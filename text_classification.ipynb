{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import collections\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender classification assignment\n",
    "\n",
    "You are to follow the instructions below and fill each cell as instructed.\n",
    "Once ready, submit this notebook on VLE with all the outputs included (run all your code and don't clear any output cells).\n",
    "Do not submit anything else apart from the notebook and do not use any extra data apart from what is provided.\n",
    "\n",
    "You will be working on classifying the genders of people from their blog posts using a data set called the [Blog Authorship Corpus](https://www.kaggle.com/rtatman/blog-authorship-corpus).\n",
    "This has been pre-split and reduced for you to use in this assignment.\n",
    "\n",
    "10% of the marks from this assignment are based on neatness.\n",
    "\n",
    "This assignment will carry 40% of the final mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing (10%)\n",
    "\n",
    "You have a train/dev/test split data set consisting of CSV files with two fields: gender and text.\n",
    "The gender field contains either 'male' or 'female' whilst the text is a string containing text from blog posts.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load these three CSV files and tokenise each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"train.csv\")\n",
    "test_set = pd.read_csv(\"test.csv\")\n",
    "dev_set = pd.read_csv(\"dev.csv\")\n",
    "\n",
    "train_set_x = train_set['text']\n",
    "train_set_y = train_set['gender']\n",
    "\n",
    "test_set_x = test_set['text']\n",
    "test_set_y = test_set['gender']\n",
    "\n",
    "dev_set_x = dev_set['text']\n",
    "dev_set_y = dev_set['gender']\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.lower()\n",
    "\n",
    "train_set_x = train_set_x.apply(preprocess)\n",
    "train_set_x = train_set_x.apply(preprocess)\n",
    "dev_set_x = dev_set_x.apply(preprocess)\n",
    "\n",
    "train_token_x = train_set_x.apply(nltk.tokenize.word_tokenize)\n",
    "test_token_x = test_set_x.apply(nltk.tokenize.word_tokenize)\n",
    "dev_token_x = dev_set_x.apply(nltk.tokenize.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that counts the number of lines in each data set as well as the maximum number of tokens in each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train_set.index)\n",
    "test_len = len(test_set.index)\n",
    "dev_len = len(dev_set.index)\n",
    "\n",
    "print(\"Train Set no. of lines:\", str(train_len))\n",
    "print(\"Test Set no. of lines:\", str(test_len))\n",
    "print(\"Dev Set no. of lines:\", str(dev_len))\n",
    "print()\n",
    "\n",
    "train_lens = [len(x) for x in train_token_x]\n",
    "train_max = max(train_lens)\n",
    "test_lens = [len(x) for x in test_token_x]\n",
    "test_max = max(test_lens)\n",
    "dev_lens = [len(x) for x in dev_token_x]\n",
    "dev_max = max(dev_lens)\n",
    "\n",
    "print(\"Train Set max tokens:\", str(train_max))\n",
    "print(\"Test Set max tokens:\", str(test_max))\n",
    "print(\"Dev Set max tokens:\", str(dev_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each data set's labels (gender) into numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = sorted(set(train_set_y))\n",
    "\n",
    "train_y_indexes = [categories.index(y) for y in train_set_y]\n",
    "test_y_indexes = [categories.index(y) for y in test_set_y]\n",
    "dev_y_indexes = [categories.index(y) for y in dev_set_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a vocabulary consisting of the tokens that occur at least 5 times in the train set and output the size of your vocabulary.\n",
    "Include the unknown token and pad token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "\n",
    "frequencies = collections.Counter(word for text in train_token_x for word in text)\n",
    "vocab = sorted(frequencies.keys(), key=frequencies.get, reverse=True)\n",
    "while frequencies[vocab[-1]] < min_freq:\n",
    "    vocab.pop()\n",
    "vocab = ['<PAD>', '<UNK>'] + sorted(vocab)\n",
    "\n",
    "print(\"Length of vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create binary bag of words feature vectors for all data set texts using the vocabulary created above (include stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocab, binary=True, analyzer=lambda text: text, dtype=np.float32)\n",
    "encoder.fit(train_set_x)\n",
    "\n",
    "train_x_vecs = encoder.transform(train_token_x).toarray()\n",
    "test_x_vecs = encoder.transform(test_token_x).toarray()\n",
    "dev_x_vecs = encoder.transform(dev_token_x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data set of indexified token sequences for all texts using the vocabulary created above, making use of unknown tokens and pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {word: i for (i, word) in enumerate(vocab)}\n",
    "\n",
    "for i in range(len(train_token_x)):\n",
    "    for j in range(len(train_token_x[i])):\n",
    "        if train_token_x[i][j] not in word2index:\n",
    "            train_token_x[i][j] = '<UNK>'\n",
    "    train_token_x[i].extend(['<PAD>']*(train_max - len(train_token_x[i])))\n",
    "    \n",
    "for i in range(len(test_token_x)):\n",
    "    for j in range(len(test_token_x[i])):\n",
    "        if test_token_x[i][j] not in word2index:\n",
    "            test_token_x[i][j] = '<UNK>'\n",
    "    test_token_x[i].extend(['<PAD>']*(test_max - len(test_token_x[i])))\n",
    "    \n",
    "for i in range(len(dev_token_x)):\n",
    "    for j in range(len(dev_token_x[i])):\n",
    "        if dev_token_x[i][j] not in word2index:\n",
    "            dev_token_x[i][j] = '<UNK>'\n",
    "    dev_token_x[i].extend(['<PAD>']*(dev_max - len(dev_token_x[i])))\n",
    "\n",
    "indexed_train_x = torch.tensor([[word2index[word] for word in text] for text in train_token_x], dtype=torch.int64)\n",
    "indexed_test_x = torch.tensor([[word2index[word] for word in text] for text in test_token_x], dtype=torch.int64)\n",
    "indexed_dev_x = torch.tensor([[word2index[word] for word in text] for text in dev_token_x], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that counts the percentage of tokens in each data set that are unknown tokens (not including pad tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_percent(tokens):\n",
    "    total_tokens = sum([len(x) for x in tokens])    \n",
    "    unk_tokens = sum([1 if word == \"<UNK>\" else 0 for text in tokens for word in text])\n",
    "    return (unk_tokens/total_tokens)\n",
    "\n",
    "train_unkper = unk_percent(train_token_x)\n",
    "test_unkper = unk_percent(test_token_x)\n",
    "dev_unkper = unk_percent(dev_token_x)\n",
    "\n",
    "print(\"Train Set Unknown %: {:.2%}\".format(train_unkper))\n",
    "print(\"Test Set Unknown %: {:.2%}\".format(test_unkper))\n",
    "print(\"Dev Set Unknown %: {:.2%}\".format(dev_unkper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression classification (20%)\n",
    "\n",
    "Write a linear regression classifier (single layer neural net) that is trained to classify the author gender from the bag of words vector of the text.\n",
    "You do not need to perform any hyperparameter tuning.\n",
    "Use L1 weight decay regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, num_categories):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.zeros((vocab_size, num_categories), dtype=torch.float32, requires_grad=True))\n",
    "        self.b = torch.nn.Parameter(torch.zeros((num_categories,), dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = Linear_Model(len(vocab), 2)\n",
    "lin_model.to('cpu')\n",
    "\n",
    "optimiser = torch.optim.Adam(lin_model.parameters())\n",
    "\n",
    "tensor_train_x_vecs = torch.tensor(train_x_vecs, dtype=torch.float32)\n",
    "tensor_train_y = torch.tensor(train_y_indexes, dtype=torch.int64)\n",
    "\n",
    "print('step', 'error')\n",
    "for step in range(1, 200+1):\n",
    "    optimiser.zero_grad()\n",
    "    output = lin_model(tensor_train_x_vecs)\n",
    "    error = torch.nn.functional.cross_entropy(output, tensor_train_y) + lin_model.w.abs().mean()\n",
    "    error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, error.detach().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy, precision, recall, and F1-score of this classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_test_x_vecs = torch.tensor(test_x_vecs, dtype=torch.float32)\n",
    "\n",
    "targets = np.array(test_y_indexes, np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_probs = torch.sigmoid(lin_model(tensor_test_x_vecs))\n",
    "    outputs = output_probs.detach().numpy().argmax(axis=1)\n",
    "\n",
    "accuracy = (targets == outputs).sum()/len(targets)\n",
    "print('accuracy: {:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that shows the top 10 tokens that are the most important for determining the author gender according to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.abs(lin_model.w.detach().numpy())\n",
    "\n",
    "category_index = 5\n",
    "weighted_words = sorted(zip(w[:, :].tolist(), vocab), reverse=True)\n",
    "top_ten = []\n",
    "\n",
    "print('Top 10 words')\n",
    "for i, weighted_word in enumerate(weighted_words[:10]):\n",
    "    m = (weighted_word[0][0] + weighted_word[0][1]) / 2\n",
    "    mean = \"{:.2%}\".format(m)\n",
    "    print(i+1,\") \",weighted_word[1],\" (\",mean,\")\",sep=\"\")\n",
    "    top_ten.append(weighted_word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that, for each data split and gender, shows the percentage of rows that include at least one of these important words (so 6 percentages in all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentageofword(word, dataset, gender):\n",
    "    count = 0\n",
    "    total = len(dataset.index)\n",
    "    for i,text in enumerate(dataset[\"text\"]):\n",
    "        text = text.lower()\n",
    "        if(dataset[\"gender\"][i] == gender):\n",
    "            if(text.find(word) != -1):\n",
    "                count += 1\n",
    "    return count/total\n",
    "\n",
    "top_word = random.randint(0,len(top_ten)-1)\n",
    "\n",
    "train_percent_male = percentageofword(top_ten[top_word], train_set, \"male\")\n",
    "train_percent_female = percentageofword(top_ten[top_word], train_set, \"female\")\n",
    "\n",
    "test_percent_male = percentageofword(top_ten[top_word], test_set, \"male\")\n",
    "test_percent_female = percentageofword(top_ten[top_word], test_set, \"female\")\n",
    "\n",
    "dev_percent_male = percentageofword(top_ten[top_word], dev_set, \"male\")\n",
    "dev_percent_female = percentageofword(top_ten[top_word], dev_set, \"female\")\n",
    "\n",
    "print(\"Word:\", top_ten[top_word], sep=\"\\t\")\n",
    "print()\n",
    "\n",
    "print(\"Train Set Male: {:.2%}\".format(train_percent_male))\n",
    "print(\"Train Set Female: {:.2%}\".format(train_percent_female))\n",
    "print(\"Test Set Male: {:.2%}\".format(test_percent_male))\n",
    "print(\"Test Set Female: {:.2%}\".format(test_percent_female))\n",
    "print(\"Dev Set Male: {:.2%}\".format(dev_percent_male))\n",
    "print(\"Dev Set Female: {:.2%}\".format(dev_percent_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning classifier (50%)\n",
    "\n",
    "Perform hyperparameter tuning on a deep learning classifier (with a convolutional neural network or a recurrent neural network) that is trained to classify the author gender from the indexified sequences of the text.\n",
    "Using the dev set for evaluation.\n",
    "Output the best hyperparameters found and do not store the best trained model as you will be training it again in the next bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, categ_size, embedding_size, window_size, hidden_size, init_dev):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embedding_matrix = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, init_dev, (vocab_size, embedding_size)), dtype=torch.float32))\n",
    "        \n",
    "        self.w1 = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, init_dev, (hidden_size, embedding_size, window_size)), dtype=torch.float32))\n",
    "        self.b1 = torch.nn.Parameter(torch.zeros((hidden_size,), dtype=torch.float32))\n",
    "        self.w2 = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, init_dev, (hidden_size, categ_size)), dtype=torch.float32))\n",
    "        self.b2 = torch.nn.Parameter(torch.zeros((categ_size,), dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x, text_lens):\n",
    "        embedded = self.embedding_matrix[x]\n",
    "        embedded_t = embedded.transpose(1, 2)\n",
    "        hidden_t = torch.nn.functional.leaky_relu(torch.nn.functional.conv1d(embedded_t, self.w1, self.b1))\n",
    "        hidden = hidden_t.transpose(1, 2)\n",
    "\n",
    "        pad_mask = torch.zeros(hidden.shape, dtype=torch.bool)\n",
    "        for i in range(hidden.shape[0]):\n",
    "            for j in range(hidden.shape[1]):\n",
    "                pad_mask[i, j, :] = j >= (text_lens[i] - self.window_size + 1)\n",
    "        \n",
    "        masked = torch.masked_fill(hidden, pad_mask, torch.tensor(np.inf))\n",
    "        pooled = torch.min(masked, dim=1)[0]\n",
    "        \n",
    "        return pooled@self.w2 + self.b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_set = [2, 3]\n",
    "window_size_set = [2, 3]\n",
    "hidden_size_set = [1, 2, 4, 8, 16]\n",
    "init_dev_set = [10.0, 1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "tensor_dev_y = torch.tensor(dev_y_indexes, dtype=torch.int64)\n",
    "\n",
    "already_generated = set()\n",
    "best_dev_acc = 0.0\n",
    "best_hyperparams = None\n",
    "for i in range(1, 20+1):\n",
    "    while True:\n",
    "        embedd_size = random.choice(embedding_size_set)\n",
    "        wind_size = random.choice(window_size_set)\n",
    "        hidden_size = random.choice(hidden_size_set)\n",
    "        init_dev = random.choice(init_dev_set)\n",
    "        hyperparams = (embedd_size, wind_size,hidden_size, init_dev)\n",
    "        if hyperparams not in already_generated:\n",
    "            already_generated.add(hyperparams)\n",
    "            break\n",
    "    if i%1 == 0:\n",
    "        print('Hyperparameter search attempt:', i)\n",
    "        print('embedding_size:', embedd_size)\n",
    "        print('window_size:', wind_size)\n",
    "        print('hidden_layer_size:', hidden_size)\n",
    "        print('init_stddev:', init_dev)\n",
    "\n",
    "    hyp_model = Conv_Model(len(vocab), len(categories), embedding_size=embedd_size, window_size=wind_size, hidden_size=hidden_size, init_dev=init_dev)\n",
    "    hyp_model.to('cpu')\n",
    "    optimiser = torch.optim.SGD(hyp_model.parameters(), lr=1.0, momentum=0.9)\n",
    "    for step in range(1, 10+1):\n",
    "        optimiser.zero_grad()\n",
    "        output = hyp_model(indexed_train_x, train_lens)\n",
    "        error = torch.nn.functional.cross_entropy(output, tensor_train_y)\n",
    "        error.backward()\n",
    "        optimiser.step()\n",
    "        print(str(step)+\", \", end=\"\")\n",
    "    print()\n",
    "    with torch.no_grad():\n",
    "        dev_acc = np.mean(np.abs(torch.sigmoid(hyp_model(indexed_dev_x, dev_lens)).detach().numpy().argmax(axis=1) - np.array(dev_y_indexes, np.int64)) < 0.1)\n",
    "    print('Dev set accuracy:', dev_acc)\n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_hyperparams = hyperparams\n",
    "        best_dev_acc = dev_acc\n",
    "        print('new best!')\n",
    "    print()\n",
    "\n",
    "(embedd_size, wind_size, hidden_size, init_dev) = best_hyperparams\n",
    "print('Best found:')\n",
    "print('embedding_size:', embedd_size)\n",
    "print('window_size:', wind_size)\n",
    "print('hidden_layer_size:', hidden_size)\n",
    "print('init_stddev:', init_dev)\n",
    "print('Dev set accuracy:', best_dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyperparameters found in the previous bit to train the classifier, this time outputting a graph showing the dev set accuracy after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_plot(x, y, max_epochs):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(x, y, color='blue')\n",
    "    plt.title('Localisation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim([0,max_epochs])\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy, precision, recall, and F1-score of this classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output a confusion matrix of the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 5 examples of correctly classified text for each gender and 5 examples of incorrectly classified text for each gender (so 20 text examples in total), all of which must be from the test set.\n",
    "This is assuming that you have at least 5 instances of each group.\n",
    "If you have less, then show whatever is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the list of important tokens determined previously (from the logistic regression classifier)?\n",
    "Write code that takes all the texts in the test set that have at least one of the important tokens and shows the percentage of these texts that were correctly classified.\n",
    "Similarly, take all the texts that don't have any of the important tokens and show the percentage of these texts that were correctly classified (so 2 percentages in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (10%)\n",
    "\n",
    "Write, in less than 300 words, your interpretation of the results and how you think the model could perform better.\n",
    "You should talk about things like overfitting/underfitting and whether the model is learning anything deep about how the different genders write or if it's just basing everything on the words used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ce3b824504692ac02849902c8b8a9757c2b44c834f8b88396ffb8e175c7b5c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
